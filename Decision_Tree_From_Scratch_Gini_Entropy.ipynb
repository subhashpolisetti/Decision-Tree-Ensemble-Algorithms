{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPoyU2GdGipz6KJ3hwKkyXH",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/subhashpolisetti/Decision-Tree-Ensemble-Algorithms/blob/main/Decision_Tree_From_Scratch_Gini_Entropy.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Decision Tree from Scratch: Gini Impurity and Entropy for Classification\n",
        "\n",
        "This notebook demonstrates how to implement a decision tree from scratch for classification tasks. The decision tree uses **Gini Impurity** and **Entropy** as impurity measures to decide the best feature and threshold for splitting the dataset. The implementation follows the standard decision tree algorithm, where the goal is to minimize the impurity of the splits at each node.\n",
        "\n",
        "## Key Concepts:\n",
        "\n",
        "1. **Gini Impurity**: A measure used to determine how often a randomly chosen element would be incorrectly classified. The formula is `1 - sum(p_i^2)` where `p_i` is the probability of each class in the node.\n",
        "   \n",
        "2. **Entropy**: Another impurity measure, often used in decision trees like ID3. It quantifies the amount of disorder or uncertainty in the data. The formula is `-sum(p_i * log2(p_i))` where `p_i` is the probability of each class in the node.\n",
        "\n",
        "3. **Information Gain**: This metric helps in choosing the best feature and threshold to split on. It calculates the reduction in impurity from the parent node to the child nodes after a split.\n",
        "\n",
        "4. **Dataset**: In this notebook, we use the **Iris dataset** from `sklearn`, which is a well-known dataset used for classification tasks. It contains 150 samples of iris flowers from three different species, with four features each.\n",
        "\n",
        "## Steps in the Notebook:\n",
        "\n",
        "1. **Data Preprocessing**: The Iris dataset is loaded, and the data is split into training and testing sets (80% for training, 20% for testing).\n",
        "\n",
        "2. **Building the Decision Tree**: The tree is built recursively using either **Gini Impurity** or **Entropy** to decide the best split at each node. The tree continues to split until it reaches the maximum depth or the stopping criteria are met (e.g., all samples belong to the same class or not enough samples to split further).\n",
        "\n",
        "3. **Training**: The decision tree is trained on the training set, and predictions are made on the test set.\n",
        "\n",
        "4. **Evaluation**: The accuracy of the model is computed using `accuracy_score` from `sklearn`.\n",
        "\n",
        "5. **Tree Visualization**: The structure of the decision tree is printed to show the feature splits at each node and the class predictions at the leaf nodes.\n",
        "\n",
        "## Libraries Used:\n",
        "- **NumPy**: For numerical computations and array manipulations.\n",
        "- **Counter**: From the `collections` module, used to count occurrences of each class in the target variable.\n",
        "- **sklearn.datasets**: For loading the Iris dataset.\n",
        "- **sklearn.model_selection**: For splitting the dataset into training and testing sets.\n",
        "- **sklearn.metrics**: For evaluating the model's accuracy.\n",
        "\n",
        "## Results:\n",
        "- The decision tree is trained on the Iris dataset and achieves **100% accuracy** on the test set.\n",
        "- The tree structure is printed, showing the feature splits and class predictions at the leaves.\n",
        "\n",
        "Feel free to modify the hyperparameters like `max_depth` to see how the tree's depth affects performance. You can also experiment with other impurity functions like **Entropy** to compare the results.\n",
        "\n",
        "---\n",
        "\n",
        "This markdown description provides an overview of the decision tree algorithm, the dataset used, and the steps in the notebook. It also outlines the libraries and results, helping users understand the notebook's purpose and flow.\n"
      ],
      "metadata": {
        "id": "dcsHul89xQ-i"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "2YvA3pubxBm-"
      },
      "outputs": [],
      "source": [
        "\n",
        "import numpy as np\n",
        "from collections import Counter\n",
        ""
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "def gini_impurity(y):\n",
        "    counts = np.bincount(y)\n",
        "    probabilities = counts / len(y)\n",
        "    return 1 - np.sum(probabilities ** 2)"
      ],
      "metadata": {
        "id": "_bPIK6YJxT1W"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "def entropy(y):\n",
        "    counts = np.bincount(y)\n",
        "    probabilities = counts / len(y)\n",
        "    return -np.sum([p * np.log2(p) for p in probabilities if p > 0])"
      ],
      "metadata": {
        "id": "nSSd8FwFxZ6S"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def information_gain(y, y_left, y_right, impurity_func):\n",
        "    p = len(y_left) / len(y)\n",
        "    return impurity_func(y) - p * impurity_func(y_left) - (1 - p) * impurity_func(y_right)"
      ],
      "metadata": {
        "id": "vzNnCByixbOd"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def split_dataset(X, y, feature_index, threshold):\n",
        "    left_indices = X[:, feature_index] <= threshold\n",
        "    right_indices = X[:, feature_index] > threshold\n",
        "    return X[left_indices], X[right_indices], y[left_indices], y[right_indices]"
      ],
      "metadata": {
        "id": "if81KGIxxdR5"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "def find_best_split(X, y, impurity_func):\n",
        "    best_gain = -1\n",
        "    best_split = None\n",
        "    n_features = X.shape[1]\n",
        "\n",
        "    for feature_index in range(n_features):\n",
        "        thresholds = np.unique(X[:, feature_index])\n",
        "        for threshold in thresholds:\n",
        "            X_left, X_right, y_left, y_right = split_dataset(X, y, feature_index, threshold)\n",
        "            if len(y_left) == 0 or len(y_right) == 0:\n",
        "                continue\n",
        "            gain = information_gain(y, y_left, y_right, impurity_func)\n",
        "            if gain > best_gain:\n",
        "                best_gain = gain\n",
        "                best_split = (feature_index, threshold)\n",
        "    return best_split"
      ],
      "metadata": {
        "id": "ZrvvCd8zxhan"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class DecisionTree:\n",
        "    def __init__(self, max_depth=None, min_samples_split=2, impurity_func=gini_impurity):\n",
        "        self.max_depth = max_depth\n",
        "        self.min_samples_split = min_samples_split\n",
        "        self.impurity_func = impurity_func\n",
        "        self.tree = None\n",
        "\n",
        "    def fit(self, X, y, depth=0):\n",
        "        if depth == self.max_depth or len(y) < self.min_samples_split or len(np.unique(y)) == 1:\n",
        "            return Counter(y).most_common(1)[0][0]\n",
        "\n",
        "        feature_index, threshold = find_best_split(X, y, self.impurity_func)\n",
        "        if feature_index is None:\n",
        "            return Counter(y).most_common(1)[0][0]\n",
        "\n",
        "        X_left, X_right, y_left, y_right = split_dataset(X, y, feature_index, threshold)\n",
        "        self.tree = {\n",
        "            \"feature_index\": feature_index,\n",
        "            \"threshold\": threshold,\n",
        "            \"left\": self.fit(X_left, y_left, depth + 1),\n",
        "            \"right\": self.fit(X_right, y_right, depth + 1)\n",
        "        }\n",
        "        return self.tree\n",
        "\n",
        "    def predict_single(self, x, tree):\n",
        "        if isinstance(tree, dict):\n",
        "            if x[tree[\"feature_index\"]] <= tree[\"threshold\"]:\n",
        "                return self.predict_single(x, tree[\"left\"])\n",
        "            else:\n",
        "                return self.predict_single(x, tree[\"right\"])\n",
        "        return tree\n",
        "\n",
        "    def predict(self, X):\n",
        "        return np.array([self.predict_single(x, self.tree) for x in X])"
      ],
      "metadata": {
        "id": "6K7ZbFJGxjS3"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.datasets import load_iris\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Load dataset\n",
        "data = load_iris()\n",
        "X, y = data.data, data.target\n",
        "\n",
        "# Split data\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Train Decision Tree\n",
        "tree = DecisionTree(max_depth=5)\n",
        "tree.fit(X_train, y_train)\n",
        "\n",
        "# Predict\n",
        "y_pred = tree.predict(X_test)\n",
        "\n",
        "# Evaluate\n",
        "print(\"Accuracy:\", accuracy_score(y_test, y_pred))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PgZwLmtQxl_F",
        "outputId": "f3a7eddd-16fe-466d-fc6c-ad50654adc9e"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy: 1.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "def print_tree(tree, depth=0):\n",
        "    if isinstance(tree, dict):\n",
        "        print(f\"{'|   ' * depth}Feature {tree['feature_index']} <= {tree['threshold']}\")\n",
        "        print_tree(tree[\"left\"], depth + 1)\n",
        "        print_tree(tree[\"right\"], depth + 1)\n",
        "    else:\n",
        "        print(f\"{'|   ' * depth}Predict: {tree}\")\n",
        "\n",
        "print_tree(tree.tree)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_HetJFSuxqBH",
        "outputId": "6a56ace1-f0aa-4473-ad3c-7d39d09577b0"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Feature 2 <= 1.9\n",
            "|   Predict: 0\n",
            "|   Feature 2 <= 4.7\n",
            "|   |   Feature 3 <= 1.6\n",
            "|   |   |   Predict: 1\n",
            "|   |   |   Predict: 2\n",
            "|   |   Feature 3 <= 1.7\n",
            "|   |   |   Feature 2 <= 4.9\n",
            "|   |   |   |   Predict: 1\n",
            "|   |   |   |   Feature 3 <= 1.5\n",
            "|   |   |   |   |   Predict: 2\n",
            "|   |   |   |   |   Predict: 1\n",
            "|   |   |   Feature 2 <= 4.8\n",
            "|   |   |   |   Feature 0 <= 5.9\n",
            "|   |   |   |   |   Predict: 1\n",
            "|   |   |   |   |   Predict: 2\n",
            "|   |   |   |   Predict: 2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "NQBwBvfjxszd"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}